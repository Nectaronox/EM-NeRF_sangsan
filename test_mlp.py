import torch
import torch.nn as nn
import numpy as np

def test_linear_vs_relu():
    """nn.Linearì™€ nn.ReLUì˜ ì°¨ì´ì ì„ ë³´ì—¬ì£¼ëŠ” í…ŒìŠ¤íŠ¸"""
    print("ğŸ” nn.Linear vs nn.ReLU ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ì…ë ¥ ë°ì´í„°
    batch_size = 4
    input_dim = 3
    x = torch.randn(batch_size, input_dim)
    print(f"ì…ë ¥ ë°ì´í„° í¬ê¸°: {x.shape}")
    print(f"ì…ë ¥ ë°ì´í„°:\n{x}")
    print()
    
    # 1. nn.Linearë§Œ ì‚¬ìš©
    linear = nn.Linear(input_dim, 5)
    output_linear = linear(x)
    print(f"ğŸŸ¦ nn.Linear(3â†’5) ê²°ê³¼:")
    print(f"ì¶œë ¥ í¬ê¸°: {output_linear.shape}")
    print(f"ì¶œë ¥ ë°ì´í„°:\n{output_linear}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in linear.parameters())}")
    print()
    
    # 2. nn.ReLUë§Œ ì‚¬ìš© (ì˜ë¯¸ ì—†ìŒ)
    relu = nn.ReLU()
    output_relu = relu(x)  # ì…ë ¥ê³¼ ë™ì¼í•œ í¬ê¸°
    print(f"ğŸŸ¨ nn.ReLU() ê²°ê³¼:")
    print(f"ì¶œë ¥ í¬ê¸°: {output_relu.shape}")
    print(f"ì¶œë ¥ ë°ì´í„°:\n{output_relu}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in relu.parameters() if p.requires_grad)}")
    print()
    
    # 3. ì˜¬ë°”ë¥¸ ì¡°í•©: nn.Linear + nn.ReLU
    mlp = nn.Sequential(
        nn.Linear(input_dim, 5),
        nn.ReLU()
    )
    output_mlp = mlp(x)
    print(f"âœ… nn.Linear + nn.ReLU ê²°ê³¼:")
    print(f"ì¶œë ¥ í¬ê¸°: {output_mlp.shape}")
    print(f"ì¶œë ¥ ë°ì´í„°:\n{output_mlp}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in mlp.parameters())}")


def analyze_nerf_mlp():
    """NeRF MLPì˜ êµ¬ì¡°ì™€ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„"""
    print("\n" + "="*50)
    print("ğŸ§  NeRF MLP êµ¬ì¡° ë¶„ì„")
    print("="*50)
    
    # NeRF ì„¤ì •
    pos_L = 10
    dir_L = 4
    hidden_dim = 256
    
    # ì…ë ¥ ì°¨ì› ê³„ì‚°
    pos_input_dim = 3 + 3 * 2 * pos_L  # 63
    dir_input_dim = 3 + 3 * 2 * dir_L  # 27
    
    print(f"ğŸ“ ìœ„ì¹˜ ì…ë ¥ ì°¨ì›: {pos_input_dim}")
    print(f"ğŸ“ ë°©í–¥ ì…ë ¥ ì°¨ì›: {dir_input_dim}")
    print()
    
    # Density Network ë¶„ì„
    density_net = nn.Sequential(
        nn.Linear(pos_input_dim, hidden_dim),  # 63 â†’ 256
        nn.ReLU(),
        nn.Linear(hidden_dim, hidden_dim),     # 256 â†’ 256
        nn.ReLU(),
        nn.Linear(hidden_dim, hidden_dim),     # 256 â†’ 256
        nn.ReLU(),
        nn.Linear(hidden_dim, hidden_dim),     # 256 â†’ 256
        nn.ReLU(),
        nn.Linear(hidden_dim, hidden_dim + 1)  # 256 â†’ 257
    )
    
    # Color Network ë¶„ì„
    color_net = nn.Sequential(
        nn.Linear(hidden_dim + dir_input_dim, hidden_dim // 2),  # 283 â†’ 128
        nn.ReLU(),
        nn.Linear(hidden_dim // 2, 3),  # 128 â†’ 3
        nn.Sigmoid()
    )
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    density_params = sum(p.numel() for p in density_net.parameters())
    color_params = sum(p.numel() for p in color_net.parameters())
    total_params = density_params + color_params
    
    print(f"ğŸ—ï¸ Density Network:")
    print(f"   - ë ˆì´ì–´ ìˆ˜: 5ê°œ")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {density_params:,}ê°œ")
    print()
    
    print(f"ğŸ¨ Color Network:")
    print(f"   - ë ˆì´ì–´ ìˆ˜: 2ê°œ")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {color_params:,}ê°œ")
    print()
    
    print(f"ğŸ“Š ì „ì²´ NeRF ë„¤íŠ¸ì›Œí¬:")
    print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ")
    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (float32): {total_params * 4 / 1024 / 1024:.2f} MB")


def demonstrate_mlp_layers():
    """MLPì˜ ê° ë ˆì´ì–´ê°€ í•˜ëŠ” ì¼ì„ ì‹œê°í™”"""
    print("\n" + "="*50)
    print("ğŸ”¬ MLP ë ˆì´ì–´ë³„ ë³€í™˜ ê³¼ì •")
    print("="*50)
    
    # ê°„ë‹¨í•œ 3ì¸µ MLP
    mlp = nn.Sequential(
        nn.Linear(3, 8),
        nn.ReLU(),
        nn.Linear(8, 4),
        nn.ReLU(),
        nn.Linear(4, 1)
    )
    
    # ì…ë ¥ ë°ì´í„°
    x = torch.tensor([[1.0, 2.0, 3.0]])
    print(f"ì…ë ¥: {x} (í¬ê¸°: {x.shape})")
    
    # ë‹¨ê³„ë³„ ë³€í™˜ ì¶”ì 
    current = x
    for i, layer in enumerate(mlp):
        current = layer(current)
        layer_name = type(layer).__name__
        print(f"ë ˆì´ì–´ {i+1} ({layer_name}): {current.detach().numpy().flatten()[:3]}... (í¬ê¸°: {current.shape})")


if __name__ == "__main__":
    test_linear_vs_relu()
    analyze_nerf_mlp()
    demonstrate_mlp_layers() 