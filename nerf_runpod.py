#!/usr/bin/env python3
"""
ê°œì„ ëœ ì „ìê¸°ì¥ NeRF - Loss ìˆ˜ë ´ ë¬¸ì œ í•´ê²°
ì£¼ìš” ê°œì„ ì‚¬í•­:
1. ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§ ë° ì •ê·œí™”
2. Gradient clipping ì¶”ê°€
3. ê°œì„ ëœ ë¬¼ë¦¬ ì œì•½ ì¡°ê±´
4. ë” ì•ˆì •ì ì¸ í›ˆë ¨ íŒŒë¼ë¯¸í„°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedSyntheticEMDataset(Dataset):
    """ê°œì„ ëœ í•©ì„± ì „ìê¸°ì¥ ë°ì´í„°ì…‹ - ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§ ì ìš©"""
    
    def __init__(self, n_samples=50000, grid_size=64, frequency_range=(1e9, 10e9)):
        self.n_samples = n_samples
        self.grid_size = grid_size
        self.frequency_range = frequency_range
        
        logger.info(f"ê°œì„ ëœ ë°ì´í„°ì…‹ ìƒì„± ì¤‘: {n_samples} ìƒ˜í”Œ")
        
        # 3D ì¢Œí‘œ ìƒì„± (ì •ê·œí™”ë¨)
        self.positions = torch.rand(n_samples, 3) * 2 - 1  # [-1, 1] ë²”ìœ„
        
        # ì£¼íŒŒìˆ˜ ìƒì„± ë° ì •ê·œí™”
        raw_frequencies = torch.rand(n_samples, 1) * (frequency_range[1] - frequency_range[0]) + frequency_range[0]
        self.frequencies = (torch.log10(raw_frequencies) - 9) / 1  # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        
        # ì‹œê°„ ìƒì„± ë° ì •ê·œí™”
        self.times = torch.rand(n_samples, 1) * 2 - 1  # [-1, 1] ë²”ìœ„
        
        # ë™ì  ê°ì²´ ì •ë³´ (ì •ê·œí™”ë¨)
        self.dynamic_objects = self.generate_dynamic_objects(n_samples)
        
        # ê°œì„ ëœ Ground truth ì „ìê¸°ì¥ (ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§)
        self.em_fields = self.simulate_improved_ground_truth()
        
        logger.info("ê°œì„ ëœ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
    
    def generate_dynamic_objects(self, n_samples):
        """ì •ê·œí™”ëœ ë™ì  ê°ì²´ ì •ë³´ ìƒì„±"""
        objects = torch.zeros(n_samples, 8)
        
        # ìœ„ì¹˜ ([-1, 1] ë²”ìœ„)
        objects[:, :3] = torch.rand(n_samples, 3) * 2 - 1
        
        # ì†ë„ ì •ê·œí™” ([-1, 1] ë²”ìœ„)
        objects[:, 3:6] = torch.rand(n_samples, 3) * 2 - 1
        
        # ê°ì²´ íƒ€ì… ì •ê·œí™” (0~1 ë²”ìœ„)
        objects[:, 6] = torch.rand(n_samples)
        
        # í¬ê¸° ì •ê·œí™” (0~1 ë²”ìœ„)
        objects[:, 7] = torch.rand(n_samples)
        
        return objects
    
    def simulate_improved_ground_truth(self):
        """ê°œì„ ëœ ë¬¼ë¦¬ ê¸°ë°˜ ground truth - ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§"""
        logger.info("ê°œì„ ëœ Ground truth ê³„ì‚° ì¤‘...")
        
        n_samples = len(self.positions)
        
        # ëª¨ë“  ê°’ë“¤ì„ [-1, 1] ë˜ëŠ” [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        E_field = torch.zeros(n_samples, 3)
        B_field = torch.zeros(n_samples, 3)
        scattering = torch.zeros(n_samples, 1)
        delay = torch.zeros(n_samples, 1)
        
        for i in range(n_samples):
            pos = self.positions[i]
            freq_norm = self.frequencies[i].item()  # ì´ë¯¸ ì •ê·œí™”ë¨
            obj = self.dynamic_objects[i]
            
            # ê±°ë¦¬ ê¸°ë°˜ ê°ì‡  (ì •ê·œí™”ë¨)
            distance = torch.norm(pos).clamp(min=0.1)
            
            # ì •ê·œí™”ëœ ì „ê¸°ì¥ ([-1, 1] ë²”ìœ„)
            E_field[i, 0] = torch.tanh(torch.sin(2 * np.pi * distance + freq_norm * np.pi))
            E_field[i, 1] = torch.tanh(torch.cos(2 * np.pi * distance + freq_norm * np.pi))
            E_field[i, 2] = torch.tanh(torch.sin(4 * np.pi * distance + freq_norm * np.pi))
            
            # ìê¸°ì¥ (Eì™€ ì¼ê´€ì„± ìˆê²Œ, ìŠ¤ì¼€ì¼ ì¡°ì •ë¨)
            B_field[i, 0] = E_field[i, 1] * 0.1  # ë¬¼ë¦¬ì  ê´€ê³„ ìœ ì§€í•˜ë˜ ìŠ¤ì¼€ì¼ ì¡°ì •
            B_field[i, 1] = -E_field[i, 0] * 0.1
            B_field[i, 2] = E_field[i, 2] * 0.1
            
            # ì‚°ë€ ê³„ìˆ˜ (0~1 ë²”ìœ„)
            obj_influence = torch.norm(obj[:3] - pos)
            scattering[i] = torch.sigmoid(2 * (obj[7] - obj_influence))
            
            # ì „íŒŒ ì§€ì—° (0~1 ë²”ìœ„)
            delay[i] = torch.sigmoid(distance + scattering[i])
        
        return {
            'electric_field': E_field,
            'magnetic_field': B_field,
            'scattering_coefficient': scattering,
            'propagation_delay': delay
        }
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        return {
            'position': self.positions[idx],
            'frequency': self.frequencies[idx],
            'time': self.times[idx],
            'dynamic_objects': self.dynamic_objects[idx],
            'ground_truth': {k: v[idx] for k, v in self.em_fields.items()}
        }

class FixedStabilizedEMNeRF(nn.Module):
    """ì°¨ì› ë¬¸ì œë¥¼ í•´ê²°í•œ ì•ˆì •í™”ëœ ì „ìê¸°ì¥ NeRF"""
    
    def __init__(self, hidden_dim=256, n_layers=8):
        super().__init__()
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ë ˆë²¨
        self.pos_encoding_levels = 8
        self.time_encoding_levels = 4
        
        # ì…ë ¥ ì°¨ì› ì •í™•íˆ ê³„ì‚°
        pos_dim = 3 + 3 * 2 * self.pos_encoding_levels  # 3 + 48 = 51
        time_dim = 1 + 1 * 2 * self.time_encoding_levels  # 1 + 8 = 9
        freq_dim = 1 + 1 * 2 * 3  # 1 + 6 = 7 (frequency_encodingì—ì„œ)
        obj_dim = 8
        
        self.input_dim = pos_dim + time_dim + freq_dim + obj_dim  # 51 + 9 + 7 + 8 = 75
        
        print(f"ê³„ì‚°ëœ ì…ë ¥ ì°¨ì›: {self.input_dim}")
        print(f"  - ìœ„ì¹˜ ì¸ì½”ë”©: {pos_dim}")
        print(f"  - ì‹œê°„ ì¸ì½”ë”©: {time_dim}")
        print(f"  - ì£¼íŒŒìˆ˜ ì¸ì½”ë”©: {freq_dim}")
        print(f"  - ë™ì  ê°ì²´: {obj_dim}")
        
        # ë‹¨ìˆœí•˜ê³  ì•ˆì „í•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
        self.layers = nn.ModuleList()
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        self.layers.append(nn.Linear(self.input_dim, hidden_dim))
        self.layers.append(nn.LayerNorm(hidden_dim))
        self.layers.append(nn.ReLU(inplace=True))
        self.layers.append(nn.Dropout(0.1))
        
        # ì¤‘ê°„ ë ˆì´ì–´ë“¤ (skip connection ì—†ì´ ë‹¨ìˆœí™”)
        for i in range(n_layers - 2):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
            self.layers.append(nn.LayerNorm(hidden_dim))
            self.layers.append(nn.ReLU(inplace=True))
            self.layers.append(nn.Dropout(0.1))
        
        # Skip connectionìš© ë ˆì´ì–´ (ì¤‘ê°„ì— ì¶”ê°€)
        skip_layer_idx = len(self.layers)
        self.layers.append(nn.Linear(hidden_dim + self.input_dim, hidden_dim))
        self.layers.append(nn.LayerNorm(hidden_dim))
        self.layers.append(nn.ReLU(inplace=True))
        self.layers.append(nn.Dropout(0.1))
        
        # ë§ˆì§€ë§‰ ë ˆì´ì–´
        self.layers.append(nn.Linear(hidden_dim, hidden_dim))
        self.layers.append(nn.LayerNorm(hidden_dim))
        self.layers.append(nn.ReLU(inplace=True))
        
        # Skip connectionì´ ì ìš©ë  ë ˆì´ì–´ ì¸ë±ìŠ¤ ì €ì¥
        self.skip_layer_idx = skip_layer_idx
        
        # ì¶œë ¥ í—¤ë“œë“¤
        self.electric_field_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 2, 3),
            nn.Tanh()
        )
        
        self.magnetic_field_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 2, 3),
            nn.Tanh()
        )
        
        self.scattering_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
        self.delay_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # Xavier ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def positional_encoding(self, x, levels):
        encoded = [x]
        for level in range(levels):
            for fn in [torch.sin, torch.cos]:
                encoded.append(fn(2**level * np.pi * x))
        return torch.cat(encoded, dim=-1)
    
    def frequency_encoding(self, freq):
        # ì´ë¯¸ ì •ê·œí™”ëœ ì£¼íŒŒìˆ˜ ì‚¬ìš©
        encoded = [freq]
        for i in range(3):
            encoded.append(torch.sin(2**i * np.pi * freq))
            encoded.append(torch.cos(2**i * np.pi * freq))
        return torch.cat(encoded, dim=-1)
    
    def forward(self, positions, frequencies, times, dynamic_objects):
        # ì¸ì½”ë”©
        pos_encoded = self.positional_encoding(positions, self.pos_encoding_levels)
        time_encoded = self.positional_encoding(times, self.time_encoding_levels)
        freq_encoded = self.frequency_encoding(frequencies)
        obj_normalized = dynamic_objects
        
        # íŠ¹ì§• ê²°í•©
        features = torch.cat([pos_encoded, time_encoded, freq_encoded, obj_normalized], dim=1)
        
        # ì°¨ì› í™•ì¸
        assert features.shape[1] == self.input_dim, f"ì…ë ¥ ì°¨ì› ë¶ˆì¼ì¹˜: {features.shape[1]} != {self.input_dim}"
        
        # ë„¤íŠ¸ì›Œí¬ í†µê³¼
        x = features
        skip_input = features
        
        for i, layer in enumerate(self.layers):
            # Skip connection ì ìš©
            if i == self.skip_layer_idx:
                x = torch.cat([x, skip_input], dim=1)
            
            x = layer(x)
        
        # ì¶œë ¥ ê³„ì‚°
        E_field = self.electric_field_head(x)
        B_field = self.magnetic_field_head(x)
        scattering = self.scattering_head(x)
        delay = self.delay_head(x)
        
        return {
            'electric_field': E_field,
            'magnetic_field': B_field,
            'scattering_coefficient': scattering,
            'propagation_delay': delay
        }

class ImprovedEMNeRFTrainer:
    """ê°œì„ ëœ NeRF íŠ¸ë ˆì´ë„ˆ - ì•ˆì •ì ì¸ í›ˆë ¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # ë” ë³´ìˆ˜ì ì¸ í•™ìŠµë¥ 
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=1e-4,  # ì¤„ì„
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.5, 
            patience=10,
            verbose=True
        )
        
        # Mixed precision
        self.scaler = torch.cuda.amp.GradScaler()
        
        # í›ˆë ¨ ê¸°ë¡
        self.train_losses = []
        self.lr_history = []
    
    def improved_physics_loss(self, predictions, targets):
        """ê°œì„ ëœ ë¬¼ë¦¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜"""
        mse_loss = nn.MSELoss()
        
        # ê¸°ë³¸ MSE ì†ì‹¤ë“¤
        e_loss = mse_loss(predictions['electric_field'], targets['electric_field'])
        b_loss = mse_loss(predictions['magnetic_field'], targets['magnetic_field'])
        s_loss = mse_loss(predictions['scattering_coefficient'], targets['scattering_coefficient'])
        d_loss = mse_loss(predictions['propagation_delay'], targets['propagation_delay'])
        
        # ê°œì„ ëœ ë¬¼ë¦¬ ì œì•½ ì¡°ê±´
        E_magnitude = torch.norm(predictions['electric_field'], dim=1)
        B_magnitude = torch.norm(predictions['magnetic_field'], dim=1)
        
        # ì •ê·œí™”ëœ ë¬¼ë¦¬ ì œì•½ (|B| â‰ˆ |E|/c, í•˜ì§€ë§Œ ì •ê·œí™”ëœ ìŠ¤ì¼€ì¼)
        # Eì™€ B ëª¨ë‘ [-1,1] ë²”ìœ„ì´ë¯€ë¡œ ë‹¨ìˆœí•œ ë¹„ë¡€ ê´€ê³„ ì‚¬ìš©
        physics_constraint = mse_loss(B_magnitude, E_magnitude * 0.1)  # ìŠ¤ì¼€ì¼ ì¡°ì •
        
        # Eì™€ Bì˜ ì§êµì„± ì œì•½ (ê°„ë‹¨í™”)
        dot_product = torch.sum(predictions['electric_field'] * predictions['magnetic_field'], dim=1)
        orthogonality_constraint = torch.mean(dot_product**2)
        
        # ê°€ì¤‘ í•©ì‚°
        total_loss = (
            e_loss + 
            b_loss + 
            s_loss + 
            d_loss + 
            0.01 * physics_constraint +  # ê°€ì¤‘ì¹˜ ì¤„ì„
            0.01 * orthogonality_constraint
        )
        
        return {
            'total': total_loss,
            'electric': e_loss,
            'magnetic': b_loss,
            'scattering': s_loss,
            'delay': d_loss,
            'physics': physics_constraint,
            'orthogonality': orthogonality_constraint
        }
    
    def train_epoch(self, dataloader):
        self.model.train()
        epoch_losses = []
        
        for batch_idx, batch in enumerate(dataloader):
            # ë°ì´í„° GPUë¡œ ì´ë™
            positions = batch['position'].to(self.device)
            frequencies = batch['frequency'].to(self.device)
            times = batch['time'].to(self.device)
            dynamic_objects = batch['dynamic_objects'].to(self.device)
            
            targets = {k: v.to(self.device) for k, v in batch['ground_truth'].items()}
            
            self.optimizer.zero_grad()
            
            # Forward pass with mixed precision
            with torch.cuda.amp.autocast():
                predictions = self.model(positions, frequencies, times, dynamic_objects)
                losses = self.improved_physics_loss(predictions, targets)
                total_loss = losses['total']
            
            # Backward pass with gradient clipping
            self.scaler.scale(total_loss).backward()
            
            # Gradient clipping ì ìš©
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            epoch_losses.append(total_loss.item())
            
            # ì •ê¸°ì ì¸ ë¡œê¹…
            if batch_idx % 50 == 0:
                logger.info(f"Batch {batch_idx}, Loss: {total_loss.item():.6f}")
                
                # ìƒì„¸ ì†ì‹¤ ì¶œë ¥
                if batch_idx % 200 == 0:
                    logger.info(f"  E-field: {losses['electric'].item():.6f}")
                    logger.info(f"  B-field: {losses['magnetic'].item():.6f}")
                    logger.info(f"  Scattering: {losses['scattering'].item():.6f}")
                    logger.info(f"  Delay: {losses['delay'].item():.6f}")
                    logger.info(f"  Physics: {losses['physics'].item():.6f}")
        
        avg_loss = np.mean(epoch_losses)
        self.train_losses.append(avg_loss)
        self.lr_history.append(self.optimizer.param_groups[0]['lr'])
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        self.scheduler.step(avg_loss)
        
        return avg_loss
    
    def plot_training_progress(self):
        """í›ˆë ¨ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss í”Œë¡¯
        ax1.semilogy(self.train_losses)
        ax1.set_title('Training Loss (Log Scale)')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True)
        
        # Learning rate í”Œë¡¯
        ax2.semilogy(self.lr_history)
        ax2.set_title('Learning Rate')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')
        plt.show()

def run_improved_training():
    """ê°œì„ ëœ í›ˆë ¨ ì‹¤í–‰"""
    logger.info("=== ê°œì„ ëœ ì „ìê¸°ì¥ NeRF í›ˆë ¨ ì‹œì‘ ===")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ê°œì„ ëœ ë°ì´í„°ì…‹
    train_dataset = ImprovedSyntheticEMDataset(n_samples=5000)  # ì‘ê²Œ ì‹œì‘
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    
    # ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ
    model = FixedStabilizedEMNeRF(hidden_dim=128, n_layers=6)  # ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì‚¬ìš©
    trainer = ImprovedEMNeRFTrainer(model, device=device)
    
    # í›ˆë ¨ ì‹¤í–‰
    n_epochs = 100
    best_loss = float('inf')
    patience_counter = 0
    patience = 20
    
    logger.info("í›ˆë ¨ ì‹œì‘...")
    for epoch in range(n_epochs):
        start_time = time.time()
        epoch_loss = trainer.train_epoch(train_loader)
        epoch_time = time.time() - start_time
        
        logger.info(f"Epoch {epoch+1}/{n_epochs}")
        logger.info(f"  Loss: {epoch_loss:.8f}")
        logger.info(f"  Time: {epoch_time:.2f}s")
        logger.info(f"  LR: {trainer.optimizer.param_groups[0]['lr']:.2e}")
        
        # Early stopping
        if epoch_loss < best_loss:
            best_loss = epoch_loss
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            logger.info(f"Early stopping at epoch {epoch+1}")
            break
        
        # ì§„í–‰ ìƒí™© ì‹œê°í™” (ë§¤ 20 ì—í¬í¬)
        if (epoch + 1) % 20 == 0:
            trainer.plot_training_progress()
    
    logger.info(f"í›ˆë ¨ ì™„ë£Œ! ìµœì¢… Loss: {best_loss:.8f}")
    
    return trainer, model

if __name__ == "__main__":
    # ê°œì„ ëœ í›ˆë ¨ ì‹¤í–‰
    trainer, model = run_improved_training()
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ¯ ê°œì„ ëœ í›ˆë ¨ ì™„ë£Œ!")
    print("="*50)
    print(f"âœ… ìµœì¢… Loss: {min(trainer.train_losses):.8f}")
    print(f"ğŸ“‰ Loss ê°œì„ : {trainer.train_losses[0]:.3e} â†’ {min(trainer.train_losses):.3e}")
    print(f"ğŸ“Š ì´ ì—í¬í¬: {len(trainer.train_losses)}")
    
    # ê¸°ëŒ€ Loss ë²”ìœ„ ì•ˆë‚´
    print("\nğŸ“‹ ì •ìƒì ì¸ Loss ë²”ìœ„:")
    print("  â€¢ ì´ˆê¸°: 1e-1 ~ 1e0")
    print("  â€¢ ì¤‘ê°„: 1e-2 ~ 1e-1") 
    print("  â€¢ ìˆ˜ë ´: 1e-3 ~ 1e-2")
    print("  â€¢ ê³¼ì í•© ì˜ì‹¬: < 1e-4")